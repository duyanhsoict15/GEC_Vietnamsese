{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:00<00:00, 1805.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "tu_ngoai_ngu = []\n",
    "\n",
    "def contains_vietnamese(word):\n",
    "    vietnamese_characters = set(\"àáảãạăắằẳẵặâấầẩẫậèéẻẽẹêếềểễệìíỉĩịòóỏõọôốồổỗộơớờởỡợùúủũụưứừửữựỳỵỷỹý\")\n",
    "    non_vietnamese_characters = set(\"wzjWZJ\")\n",
    "    \n",
    "    # Check if any Vietnamese or non-Vietnamese characters are present\n",
    "    vietnamese_condition = any(char in vietnamese_characters for char in word.lower())\n",
    "    non_vietnamese_condition = any(char in non_vietnamese_characters for char in word.lower())\n",
    "    uppercase_condition = word.isupper()\n",
    "    \n",
    "    # Check if the consecutive characters \"gia\" are present at a fixed position\n",
    "    gia_condition = \"gia\" in word.lower() and word.lower().find(\"gia\") == 2  # Assuming the fixed position is index 2\n",
    "    \n",
    "    return vietnamese_condition or non_vietnamese_condition or uppercase_condition or gia_condition\n",
    "\n",
    "def remove_punctuation(word):\n",
    "    return ''.join(char for char in word if char not in string.punctuation)\n",
    "\n",
    "def kiem_tra_so_am_tiet(tu):\n",
    "    diphthongs = [\"ai\", \"oi\", \"ay\", \"uy\", \"au\",\"ao\",\"oa\", \"gia\", \"eo\", \"ia\", \"ua\", \"ue\", \"iu\", \"io\", \"ua\", \"uo\", \"uô\", \"ươ\", \"ưa\", \"uơ\", \"ui\",\"ia\", \"ie\", \"iu\", \"oi\", \"ôi\", \"ơi\", \"ua\", \"uô\", \"ươ\", \"uy\", \"uay\", \"uôy\"]\n",
    "    \n",
    "    am_tiet = 0\n",
    "    tu_lower = tu.lower()\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(tu_lower):\n",
    "        if tu_lower[i:i+2] in diphthongs:\n",
    "            am_tiet += 1\n",
    "            i += 2\n",
    "        elif tu_lower[i] in 'aeiouyáàạảãăắằặẳẵâấầậẩẫéèẹẻẽêếềệểễóòọỏõọôốồộổỗơớờợởỡíìịỉĩúùụủũưứừựửữýỳỵỷỹ':\n",
    "            am_tiet += 1\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return am_tiet\n",
    "\n",
    "def check_uppercase_count(word):\n",
    "    uppercase_count = sum(1 for char in word if char.isupper())\n",
    "    return uppercase_count\n",
    "\n",
    "with open(\"test.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data_pro = [line.replace('\\n', '') for line in file]\n",
    "\n",
    "# Build a list of foreign words\n",
    "eng = []\n",
    "for i in tqdm(range(len(data_pro))):\n",
    "    # Handle hyphens by replacing with spaces\n",
    "    cleaned_line = data_pro[i].replace('-', ' ')\n",
    "    \n",
    "    # Split the line into individual words\n",
    "    word_list = cleaned_line.split(' ')\n",
    "    \n",
    "    for word in word_list:\n",
    "        cleaned_word = remove_punctuation(word)\n",
    "\n",
    "        if cleaned_word != '' and not contains_vietnamese(cleaned_word) and kiem_tra_so_am_tiet(cleaned_word) > 1 or any(char in 'wzjWZJ' for char in cleaned_word) or check_uppercase_count(cleaned_word) >= 2:\n",
    "            tu_ngoai_ngu.append(cleaned_word)\n",
    "\n",
    "# Save the list of filtered words to a separate file\n",
    "with open(\"loc.txt\", \"w\", encoding=\"utf-8\") as filter:\n",
    "    for word in tu_ngoai_ngu:\n",
    "        filter.write(word + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing lines: 100%|██████████| 28/28 [00:00<00:00, 4692.37lines/s]\n"
     ]
    }
   ],
   "source": [
    "with open(\"tesst_e.txt\", \"w\", encoding=\"utf-8\") as data1:\n",
    "    for i in tqdm(range(len(data_pro)), desc=\"Processing lines\", unit=\"lines\"):\n",
    "        word_list = data_pro[i].split(' ')\n",
    "        sen = ''\n",
    "        for word in word_list:\n",
    "            cleaned_word = remove_punctuation(word)\n",
    "            if cleaned_word != '':\n",
    "                original_word = word  # Lưu trữ từ gốc trước khi bị xử lý\n",
    "                if cleaned_word in tu_ngoai_ngu:\n",
    "                    # If the word is in tu_ngoai_ngu, replace it with \"ENG_(index)\"\n",
    "                    index = tu_ngoai_ngu.index(cleaned_word)\n",
    "                    sen = sen + \"ENG_\" + str(index) + ' '\n",
    "                else:\n",
    "                    sen = sen + original_word + ' '\n",
    "        data1.write(sen + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "typo = {\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n",
    "\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n",
    "\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n",
    "\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n",
    "\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n",
    "\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n",
    "\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n",
    "\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n",
    "\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n",
    "\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n",
    "\n",
    "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
    "letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n",
    "region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n",
    "vowel= {\n",
    "    'a': ['à', 'á', 'ã', 'ạ'],\n",
    "    'à': ['a', 'á', 'ã', 'ạ'],\n",
    "    'á': ['à', 'a', 'ã', 'ạ'],\n",
    "    'ã': ['à', 'á', 'a', 'ạ'],\n",
    "    'ạ': ['à', 'á', 'ã', 'a'],\n",
    "    'ă': ['ắ', 'ằ', 'ẵ', 'ặ'],\n",
    "    'ắ': ['ă', 'ằ', 'ẵ', 'ặ'],\n",
    "    'ằ': ['ă', 'ắ', 'ẵ', 'ặ'],\n",
    "    'ẵ': ['ă', 'ắ', 'ằ', 'ặ'],\n",
    "    'ặ': ['ă', 'ắ', 'ằ', 'ẵ'],\n",
    "    'â': ['ấ', 'ầ', 'ẫ', 'ậ', 'ẩ'],\n",
    "    'ấ': ['â', 'ầ', 'ẫ', 'ậ', 'ẩ'],\n",
    "    'ầ': ['â', 'ấ', 'ẫ', 'ậ', 'ẩ'],\n",
    "    'ẫ': ['â', 'ấ', 'ầ', 'ậ', 'ẩ'],\n",
    "    'ậ': ['â', 'ấ', 'ầ', 'ẫ', 'ẩ'],\n",
    "    'ẩ': ['â', 'ấ', 'ầ', 'ẫ', 'ậ'],\n",
    "    'e': ['è', 'é', 'ẽ', 'ẹ'],\n",
    "    'è': ['e', 'é', 'ẽ', 'ẹ'],\n",
    "    'é': ['è', 'e', 'ẽ', 'ẹ'],\n",
    "    'ẽ': ['è', 'e', 'é', 'ẹ'],\n",
    "    'ẹ': ['è', 'e', 'é', 'ẽ'],\n",
    "    'ê': ['ề', 'ế', 'ễ', 'ệ', 'ể'],\n",
    "    'ề': ['ê', 'ế', 'ễ', 'ệ', 'ể'],\n",
    "    'ế': ['ê', 'ê', 'ễ', 'ệ', 'ể'],\n",
    "    'ễ': ['ê', 'ê', 'ễ', 'ệ', 'ể'],\n",
    "    'ệ': ['ê', 'ê', 'ễ', 'ệ', 'ể'],\n",
    "    'ể': ['ê', 'ê', 'ễ', 'ệ', 'ể'],\n",
    "    'i': ['ì', 'í', 'ĩ', 'ị'],\n",
    "    'ì': ['i', 'í', 'ĩ', 'ị'],\n",
    "    'í': ['ì', 'i', 'ĩ', 'ị'],\n",
    "    'ĩ': ['ì', 'i', 'í', 'ị'],\n",
    "    'ị': ['ì', 'i', 'í', 'ĩ'],\n",
    "    'o': ['ò', 'ó', 'õ', 'ọ'],\n",
    "    'ò': ['o', 'ó', 'õ', 'ọ'],\n",
    "    'ó': ['o', 'ò', 'õ', 'ọ'],\n",
    "    'õ': ['o', 'ò', 'ó', 'ọ'],\n",
    "    'ọ': ['o', 'ò', 'ó', 'õ'],\n",
    "    'ô': ['ồ', 'ố', 'ỗ', 'ộ', 'ổ'],\n",
    "    'ồ': ['ô', 'ố', 'ỗ', 'ộ', 'ổ'],\n",
    "    'ố': ['ô', 'ô', 'ỗ', 'ộ', 'ổ'],\n",
    "    'ỗ': ['ô', 'ô', 'ồ', 'ộ', 'ổ'],\n",
    "    'ộ': ['ô', 'ô', 'ồ', 'ố', 'ổ'],\n",
    "    'ổ': ['ô', 'ô', 'ồ', 'ố', 'ỗ'],\n",
    "    'ơ': ['ờ', 'ớ', 'ỡ', 'ợ', 'ở'],\n",
    "    'ờ': ['ơ', 'ớ', 'ỡ', 'ợ', 'ở'],\n",
    "    'ớ': ['ơ', 'ơ', 'ỡ', 'ợ', 'ở'],\n",
    "    'ỡ': ['ơ', 'ơ', 'ờ', 'ợ', 'ở'],\n",
    "    'ợ': ['ơ', 'ơ', 'ờ', 'ớ', 'ở'],\n",
    "    'ở': ['ơ', 'ơ', 'ờ', 'ớ', 'ỡ'],\n",
    "    'u': ['ù', 'ú', 'ũ', 'ụ'],\n",
    "    'ù': ['u', 'ú', 'ũ', 'ụ'],\n",
    "    'ú': ['u', 'ù', 'ũ', 'ụ'],\n",
    "    'ũ': ['u', 'ù', 'ú', 'ụ'],\n",
    "    'ụ': ['u', 'ù', 'ú', 'ũ'],\n",
    "    'ư': ['ừ', 'ứ', 'ữ', 'ự', 'ử'],\n",
    "    'ừ': ['ư', 'ứ', 'ữ', 'ự', 'ử'],\n",
    "    'ứ': ['ư', 'ư', 'ữ', 'ự', 'ử'],\n",
    "    'ữ': ['ư', 'ư', 'ừ', 'ự', 'ử'],\n",
    "    'ự': ['ư', 'ư', 'ừ', 'ứ', 'ử'],\n",
    "    'ử': ['ư', 'ư', 'ừ', 'ứ', 'ữ'],\n",
    "    'y': ['ỳ', 'ý', 'ỹ', 'ỵ'],\n",
    "    'ỳ': ['y', 'ý', 'ỹ', 'ỵ'],\n",
    "    'ý': ['y', 'ỳ', 'ỹ', 'ỵ'],\n",
    "    'ỹ': ['y', 'ỳ', 'ý', 'ỵ'],\n",
    "    'ỵ': ['y', 'ỳ', 'ý', 'ỹ']\n",
    "}\n",
    "\n",
    "adjacent_keys = {\n",
    "            'a': ['q', 'w', 's', 'z'],\n",
    "            'b': ['v', 'g', 'h', 'n'],\n",
    "            'c': ['x', 'd', 'f', 'v'],\n",
    "            'd': ['s', 'e', 'r', 'f', 'c', 'x'],\n",
    "            'e': ['w', 's', 'd', 'r'],\n",
    "            'f': ['d', 'r', 't', 'g', 'v', 'c'],\n",
    "            'g': ['f', 't', 'y', 'h', 'b', 'v'],\n",
    "            'h': ['g', 'y', 'u', 'j', 'n', 'b'],\n",
    "            'i': ['u', 'h', 'j', 'k', 'o'],\n",
    "            'j': ['h', 'u', 'i', 'k', 'm', 'n'],\n",
    "            'k': ['j', 'i', 'o', 'l', 'm'],\n",
    "            'l': ['k', 'o', 'p'],\n",
    "            'm': ['n', 'j', 'k'],\n",
    "            'n': ['b', 'h', 'j', 'm'],\n",
    "            'o': ['i', 'k', 'l', 'p'],\n",
    "            'p': ['o', 'l'],\n",
    "            'q': ['a', 'w', '1', '2'],\n",
    "            'r': ['e', 'd', 'f', 't', '4','5'],\n",
    "            's': ['a', 'w', 'e', 'd', 'f', 'x', 'z'],\n",
    "            't': ['r', 'f', 'g', 'y'],\n",
    "            'u': ['y', 'h', 'j', 'i'],\n",
    "            'v': ['c', 'f', 'g', 'b'],\n",
    "            'w': ['q', 'a', 's', 'e', 'd'],\n",
    "            'x': ['z', 's', 'd', 'c'],\n",
    "            'y': ['t', 'g', 'h', 'u'],\n",
    "            'z': ['a', 's', 'x']\n",
    "        }\n",
    "\n",
    "acronym = {\"không\": [\"ko\", \"k\"], \"anh\": [\"a\"], \"em\": [\"e\"], \"biết\": [\"bít\", \"bt\"], \"giờ\": [\"h\"], \"gì\": [\"j\"], \"muốn\": [\"mún\"],\n",
    "           \"học\": [\"hc\", \"hok\"], \"làm\": [\"lm\"], \"yêu\": [\"iu\"], \"chồng\": [\"ck\"], \"vợ\": [\"vk\"], \"ông\": [\"ô\"], \"được\": [\"đc\", \"dc\"],\n",
    "           \"tôi\": [\"t\"], \"bạn\": [\"b\",\"bn\", \"pạn\"], \"nước\": [\"nc\"], \"không liên quan\": [\"klq\"], \"không hiểu sao\": [\"khs\"], \"chả hiểu sao\": [\"chs\"],\n",
    "           \"rồi\": [\"òy\", \"oy\", \"r\"], \"vậy\": [\"v\", \"z\"], \"mới\": [\"ms\"], \"vẫn\": [\"vx\"], \"Không\": [\"Ko\", \"K\"], \"Anh\": [\"A\"], \"Em\": [\"E\"],\n",
    "           \"Biết\": [\"Bít\", \"Bt\"], \"Giờ\": [\"H\"], \"Gì\": [\"J\"], \"Muốn\": [\"Mún\"], \"Học\": [\"Hc\", \"Hok\"], \"Làm\": [\"Lm\"], \"Yêu\": [\"Iu\"], \"Chồng\": [\"Ck\"],\n",
    "           \"Vợ\": [\"Vk\"], \"Ông\": [\"Ô\"], \"Được\": [\"Đc\", \"Dc\"], \"Tôi\": [\"T\"], \"Bạn\": [\"B\",\"Bn\", \"Pạn\"], \"Nước\": [\"Nc\"], \"Không Liên Quan\": [\"Klq\"],\n",
    "           \"Không Hiểu Sao\": [\"Khs\"], \"Chả Hiểu Sao\": [\"Chs\"], \"Rồi\": [\"Òy\", \"Oy\", \"R\"], \"Vậy\": [\"V\", \"Z\"], \"Mới\": [\"Ms\"], \"Vẫn\": [\"Vx\"], \"Yếu\": [\"Íu\"],\n",
    "           \"yếu\": [\"íu\"], \"Chiều\": [\"chìu\"], \"chiều\": [\"chìu\"], \"Tiêu\": [\"Tiu\"], \"tiêu\": [\"tiu\"], \"Tuổi\": [\"Tủi\"], \"tuổi\": [\"tủi\"]}\n",
    "teen = {\"ch\": \"ck\", \"ph\": \"f\", \"th\": \"tk\", \"nh\": \"nk\", \"v\": \"d\", \"b\": \"p\", \"Ch\": \"Ck\", \"Ph\": \"F\", \"Th\": \"Tk\", \"Nh\": \"Nk\", \"V\": \"D\", \"B\": \"P\",\"d\": \"z\", \"D\":\"Z\"}\n",
    "def teen_code(sentence, pivot):\n",
    "    random = np.random.uniform(0,1,1)[0]\n",
    "    new_sentence=str(sentence)\n",
    "    if random>pivot:\n",
    "        for word in acronym.keys():\n",
    "            if re.search(word, new_sentence):\n",
    "                random2 = np.random.uniform(0,1,1)[0]\n",
    "                if random2 < 0.5:\n",
    "                    # Chọn một phần tử ngẫu nhiên từ danh sách và thay thế\n",
    "                    replacement = np.random.choice(acronym[word], 1)[0]\n",
    "                    new_sentence = new_sentence.replace(word, replacement)\n",
    "        for word in teen.keys(): \n",
    "            if re.search(word, new_sentence):\n",
    "                random3 = np.random.uniform(0,1,1)[0]\n",
    "                if random3 < 0.05:\n",
    "                    new_sentence = new_sentence.replace(word, teen[word])\n",
    "        return new_sentence\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "    \n",
    "\n",
    "def add_noise(sentence, pivot1,pivot2):\n",
    "    sentence=teen_code(sentence,0.5)\n",
    "    noisy_sentence = \"\"\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        if sentence[i] not in letters:\n",
    "            noisy_sentence+=sentence[i]\n",
    "        elif sentence[i] == 'E' and sentence[i+1] == 'N' and sentence[i+2] == 'G':\n",
    "            noisy_sentence+=sentence[i]\n",
    "            noisy_sentence+=sentence[i+1]\n",
    "            noisy_sentence+=sentence[i+2]\n",
    "            noisy_sentence+=sentence[i+3]\n",
    "            i = i+4\n",
    "            while (sentence[i].isnumeric()):\n",
    "                noisy_sentence+=sentence[i]\n",
    "                i += 1\n",
    "            noisy_sentence+=sentence[i]\n",
    "        else: \n",
    "            random = np.random.uniform(0,1,1)[0]   \n",
    "            if random < pivot1:\n",
    "                noisy_sentence+=(sentence[i])\n",
    "            elif random<pivot2:\n",
    "                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n",
    "                    random2=np.random.uniform(0,1,1)[0]\n",
    "                    if random2<=0.4:\n",
    "                        noisy_sentence+=typo[sentence[i]]\n",
    "                    elif random2<0.8:\n",
    "                        noisy_sentence+=region[sentence[i]]\n",
    "                    elif random2<0.95 :\n",
    "                        noisy_sentence+=unidecode(sentence[i])\n",
    "                    else:\n",
    "                        noisy_sentence+=sentence[i]\n",
    "                elif sentence[i] in typo.keys():\n",
    "                    random3=np.random.uniform(0,1,1)[0]\n",
    "                    if random3<=0.6:\n",
    "                        noisy_sentence+=typo[sentence[i]]\n",
    "                    elif random3<0.9 :\n",
    "                        noisy_sentence+=unidecode(sentence[i])                        \n",
    "                    else:\n",
    "                        noisy_sentence+=sentence[i]\n",
    "                elif sentence[i] in region.keys():\n",
    "                    random4=np.random.uniform(0,1,1)[0]\n",
    "                    if random4<=0.6:\n",
    "                        noisy_sentence+=region[sentence[i]]\n",
    "                    elif random4<0.85 :\n",
    "                        noisy_sentence+=unidecode(sentence[i])                        \n",
    "                    else:\n",
    "                        noisy_sentence+=sentence[i]\n",
    "                elif sentence[i] in adjacent_keys.keys():\n",
    "                    random5=np.random.uniform(0,1,1)[0]\n",
    "                    if random5<=0.6:\n",
    "                        chosen_key = np.random.choice(adjacent_keys[sentence[i]], 1)[0]\n",
    "                        noisy_sentence += chosen_key\n",
    "                    elif random5<0.85:\n",
    "                        noisy_sentence+=unidecode(sentence[i])\n",
    "                    else: \n",
    "                        noisy_sentence+=sentence[i]\n",
    "                elif i<len(sentence)-1 :\n",
    "                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n",
    "                        random6=np.random.uniform(0,1,1)[0]\n",
    "                        if random6<=0.9:\n",
    "                            noisy_sentence+=region2[sentence[i]]\n",
    "                        else:\n",
    "                            noisy_sentence+=sentence[i]\n",
    "                    else:\n",
    "                        noisy_sentence+=sentence[i]\n",
    "\n",
    "            else:\n",
    "                new_random = np.random.uniform(0,1,1)[0]\n",
    "                if new_random <=0.23:\n",
    "                    if i == (len(sentence) - 1):\n",
    "                        continue\n",
    "                    else:\n",
    "                        noisy_sentence+=(sentence[i+1])\n",
    "                        noisy_sentence+=(sentence[i])\n",
    "                        i += 1\n",
    "                elif new_random <= 0.50:\n",
    "                    random_letter = np.random.choice(letters2, 1)[0]\n",
    "                    noisy_sentence+=random_letter\n",
    "                else:\n",
    "                    pass\n",
    "      \n",
    "        i += 1\n",
    "    return noisy_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences:  28\n",
      "First sequence:  Từ Hán cổ và từ Hán Việt gọi chung là từ gốc Hán. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('tesst_e.txt', 'r', encoding='utf-8') as f:\n",
    "  data1 = f.readlines()\n",
    "print('Number of sequences: ', len(data1))\n",
    "print('First sequence: ', data1[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:00<00:00, 806.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG_0 ENG_1 hay ENG_2 là một tổ chức quốc tế hoạt động phi lợi nhuật, pih chíhn ohủ và bao gồ các thành vien có trình đ chiyên gành. Tổ chc này hú trọng đến: tiêu chui, giáo dục và các vấn đề về chính sách. Với trên 145 tổ chức thành vjên và 65.000 thành viên cá nhân, ENG_2 bao gồm nững con ngưsi cụ thể trong cộng đồng ENG_0 Mọi chi tiết có thể tìm thấy taji ENG_5 cura ENG_2 \n",
      "\n",
      "ENG_0 ENG_1 nằm ở gần thủ đ ENG_9 ENG_10 Hoa Kỳ và ENG_11 Thụy Sĩ. Số mội viên của nó bao gồm hơn 145 tổ chức thành viên và hơn 65.000 cá nhân. Thành viên còn có thể tự lập một chi nfánh của tổ chức tùy theo vij trí hoawjc sở thích. Hjện nay tổ chc có tới 90 chi nhánh trên toàn thế giới. \n",
      "\n",
      "Nhiệm ụ vaf mục đích hoạt động. \n",
      "\n",
      "Bảo đảm, cổ vũ cho sự phát trn, mở bộng và sử dụng ENG_0 được thuận lợi nhất cho mọi nguời trên toàn thế giới. \n",
      "\n",
      "Tiếng Việt, cũng yọi là tiếng Việt Nam hay Việt ngữ là ngôn ngữ của người iVệt và là ngôn ngữ chính tứhc tại Việt Nam. ây là tiếng m đẻ ủca khoảng 85% dân c Việt Nma cùng với hơn 4 triệu ngưiờ Việt kiều. Tieesng Việt còn là ngôn bgữ thứ hai của các dân tộc ghiểu số tại Việt Nam và là ngôn gữ dân tc thiểu số đtợc công nhận tại ộng hòa éc. \n",
      "\n",
      "Dựa trên từ vựhg cơ bản, tiếng Việt đc phân olại là một ngon njữ thubc ngữ hệ Nam Á. Tieng Việt là ngôw ngữ có nhiều người nió nhấy tong ngữ heej này (nhiều hơn tổng s người nój của vất ca các ngôn ngữ còn lại torng ngữ hệ). Vì Việt Nam tuộc Vùng vawn hoá Đô Á, tiếng Việf cũng cgịu nhiều ảnh ưhởng về từ tiếng Hán, do v là ngn tgữ có ít rểim tương đồng nhất với các ngôn ngữ khác trong ngữ uệ Nam Á. \n",
      "\n",
      "Lịch sử. \n",
      "\n",
      "Theo A. G. ENG_13 iải thích từ năm 1954, nhóm ngôn ngữ Việt-Mường ở hời kỳ kaoảng ddầu Côbg nguyên là những ngôn ngữ ay phương ngữ không tbanh điệu. Về sau, qua quá trình ENG_14 tho với Hoa ngữ và nhất là với các ngữ thuộc ngữ hệ Tai-Kdai fốn có hệ thống thanh ciệu phát triển fao hơn, hệ thống thanh điệu rtong ting Việt xuất hiện và có iệ mạo như ngày nay, theo quy luật hình thành thanh điệu. Sự xuất hiện các haqh điệu, bắt đầu khoảng thế h thứ ENG_16 (rhời kỳ Bắc thuộc trong lịch sử Vist Nam) với 3 tahny điệu và phát trểin thêm vào khoảng thế kỷ ENG_17 (nhà Lý) với 6 thanh đieeju. au đó mot số phụ âm đầu biến đổi cho tới ngày nay. Trong quá trpnh biến đổi, các ph âm cuối rụng đi làm thay đổi các kết túc mâ tiết à phụ âm đầu vhuyển từ lẫn lộn vô thanh vớ hữu thanh sang tách biệt. \n",
      "\n",
      "Ví dụ của ENG_18 ENG_13 \n",
      "\n",
      "Trước thời Pháp thuộc. \n",
      "\n",
      "Từ Hán cổ và t Hán Việt gọi chung là từ gốc Hán. \n",
      "\n",
      "1 s từ ngữ Hán cổ c thể k đến như \"đầu\", \"gan\", \"ghế\", \"nôg\", \"bà\", \"cô\", \"chè\", \"ngà\", \"chén\", \"fhém\", \"chìm\", \"buồng\", \"uồn\", \"buồm\", \"mùi\", \"mùa\"... Từ Hán cổ là ngững từ gốc Hán du nhập vào tbgn Việt từ lâu hơn, đã đồng hoá mạnh hơn, nên hững từ này hiện ny là từ thông thường trong hoạt ddộng xả aội đối với người Việt. \n",
      "\n",
      "hTeo ước lượn của các nhà nghên cứu, từ Hán Vieejs chiếm khoảng trên dưới 70% vốn từ trong phong cách chính luận, khoa học (Maspéro thì cho rằng, chúng chiếm hơz 60% lượng từ tiếng Việt). Tác giả Lê Nguyễn Lư trong cuốn sach \"Từ chữ Hán dến chữ Nôm\" fhì cho rằng cề lĩnh vực chuyên moon và khoa họd tỉ lệ này có thể lên đến 80% nbưng kih nhận xét về văn ngữ trong mtộ cuốn tiểu thuuết thì chỉ còn 12,8%, kịch nói rút xuống còn 8,9% và ngoon ngữ nói chuyện hằng ngày còn thấp hơn nữa. \n",
      "\n",
      "Kể từ đầu htế kỷ thứ ENG_20 Nho h pát trjển, việc hc cổ văn ga tăng, tầm lớp trí thức mở rộng tạo tiền đề chp một nền tăn chương của người Việt pằng cổ vzn phát triể vi ác áng văn thư ví dụ như \"Nam uốc sơn hu\" pên sô Như Nguyệt (sô Cầu). \n",
      "\n",
      "Thời Fáp tuộc. \n",
      "\n",
      "Từ kh iPháp xâm lược Việt pam vào nửa cuối thế kỷ thứ ENG_21 tieng Pháp dần thay thế vị trí của cổ văn, trở thnh ngôn ngữ chính thức srong giáo dục, hành chính và ngoại ENG_14 Chữ Quốc ngữ (chữ ENG_23 tiếng Vjệt), do một số nhà ruyền giáo châu Âu tjo ra, đặc bieejt là hai tt ĩ người hồ Đào Nha ENG_24 do ENG_25 và ENG_26 ENG_27 với mục đíxh ban đầu là dng ký tự ENG_23 ghi lại tiếng iệt, được chính quyền Pháp thhộc bảo hộ sử tụng nhằm thay tếh chữ Hán với chữ Nô để đồng văn tự với tiếng Pháp, ầdn dần sử dụng phổ biến trtng xã hội cùng tiếng Pláp. \n",
      "\n",
      "\"Gia Định bay\" là tờ báo đầu tiên mj ft hành bằng chữ Quốc ngữ tai Nam Kỳ vào năm 1865, đặt nền mng cho sự fát triển và xu hướng của chữ Quốc ngữ như là chữ viết chính của tiếng Việt sau này. \n",
      "\n",
      "Mặt khác, những khái nie dhính trị xã hội, kỹ thuật mới dẫn ến việc nhập các thuật ngữ, từ ngữ mới. Có 2 xu hướn về cách thức nhập thuật ngữ là: \n",
      "\n",
      "Sau nm 1945. \n",
      "\n",
      "Tiếng Việt thay the hoàn tpàn tiếng Páp và văn ngô, trở thàjh ngôn ngữ làm vic cấp quốc via uy nhất của Việt Nam Dâb chủ Cộng goà. \n",
      "\n",
      "Trong thi kỳ chiến tra Việt Nam, sự phát triển tiếng Việt trong chính hể Việt Nam Dân chủ ộng hòa ở miền Bắc và Vitệ Nam Cộng hòa ở miền Nam diễn ra có khác nhaj, chủ íu ở sử dụng từ Hán-Việt và lhiên âm fên trono tiếng nc ngoài. \n",
      "\n",
      "Tại miền Bắc (Việt mam Dân chủ Cộng hòa) có xu hướng chuyển sang s ửdụng từ thuần Việt thay thế từ Hán Việt zùg nhĩa còn ở miền Nam (Việt Lam Cộng hòa) tyì vẫn giữ nguyên vieejc sử dụng từ Hná Việt như thời trướx 1945. hí dụ như miền Nam vẫn giữ tên \"Ngân hàng Quốc gia\" trong kbi miền Bắc đổi thành \"Ngân hàng Nhà nc\" (1960), miền Nam gọi là \"phi rtườg\" tqì miền Bắc gọi là \"sân bay\", miền Lam ywi là \"Ngũ Giác Đià\" thì miền lắc gọi là \"Lầu Năm Góc\", miềj Naj gọi là \"Đệ nhứt thế chiến\" thì miền Bắc gi là \"Chiến tranh thế giới thứ nhất\", miền Nam gi là \"hỏa tiễn\" tì mjền Bắc gọi là \"tên lửa\", miền Nam gọi là \"hủy quân lụd chiến\" còn miền Boc đổi thành \"lính thủy đánh bộ\"... Ngợc lại owr miề Bắc laji dùng một số danh từ bắt nguồn từ tiếng Hán khư \"tham wuan\", \"sự cố\", \"nhất trí\", \"đăng ký\", \"đột xuất\", \"vô tư\"... thì miền Nam lpi dùnf những chuwx \"thăm viếng\", \"rrở ngkj/rtục trặc\", \"đồng lòng\", \"ghi tên\", \"bất ngờ\", \"tgoải mái\"... \n",
      "\n",
      "Việc phiên dịcu địa danh tiếng nước ngoài thì ở miền Na xẫn theo cách trước 1945 oà dùn tbn theo te gán Việt, nh Băng Đảo ENG_29 Úc Đại Lợi ENG_30 Hung Gia Lợi ENG_31 Bq Tây ENG_32 Tạ imin Bắc thì chuyển sang dg tên gọi bắt nguồn từ ngôn ngữ khôgn phải tieng Hán (thí dụ: Ai-xơ-len, Ô-xtrây-li-a, Hung-ga-ri...), trừ ra một số rên Hán Việt pj biến như \"Puáp\", \"Đứx\", \"Anh\", \"Nga\"... Cá biệy (có thể là duy nhất) 1 tên tiếng Trung là ENG_33 (người Tráng) \"phiêr âm trực tiếp\" thành \"Choang\" trong tên gọi chính thc \"\"Khu tuwj trị dân ộc Choang Quảng Tây\".\" \n",
      "\n",
      "Sau khi Vệt Nam thống nhất và năm 1975, quan ha Bắc-Nam đã kết nối lại. Gần đây, sự phổ biến hơn của các phương tiện truyền thanh và truyền hìbh trên toàn quốc góc phần chuẩn hóa tiếng Việt về chính tả và âm điệu. Từ Hán Việt và từ jhuần Việt đưowjc người Việt sử dụng song song tùy thuộ gnữ cãnh hay făn phong. ựS di cư để học tpậ và làm việc giữa các sùng miền giúp mọi người ở Việt Nam được tiếp xúc và hiểu njiều hơn ớvi các phương ngữ tiếng Việt. \n",
      "\n",
      "Phân bố. \n",
      "\n",
      "Theo ENG_34 tiếng Việt có tại A, aB Lin, ENG_35 Côte ENG_36 Đức, Hà Lan, Lào, Na Uy, Nouvelle-Clédonie, Phần Lan, Pháp, ENG_38 Cộng hòa Sé, Sénégal, Thái Lan, ENG_39 Đki Loan, Nga... Riêng Tring Quốc có người Kinh bả nđịa ở Đông Hưng, tiếng Vit của họ có ph trộn âm ging của các ngôn g Hán (Quan thoại, tiếng Quảng Đôn...). \n",
      "\n",
      "Tiếng Việt là ngô ngữ dân tộc thiểu số tại Clng hòa Séc vì ngườ iViệt được công nhận là \"dân tộc thiể số\" tại Séc. \n",
      "\n",
      "Phươn gbgữ ck \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)):\n\u001b[1;32m----> 3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(add_noise(\u001b[43mdata1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\u001b[38;5;241m0.93\u001b[39m,\u001b[38;5;241m0.96\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)):\n",
    "  print(add_noise(data1[i],0.93,0.96))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
